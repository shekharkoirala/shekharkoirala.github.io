[{"content":"MOTIVE So, Ubuntu 24.04 LTS was released on 25 April 2024. Its been more 3 months from the release and I thought it would be safe enough to install it in my main working computer. Wrong!! I wanted a documentation for setting up the CUDA 12.2 in Ubuntu 24.04 but ended for reinstalling everything. Thus, here is the blog for future me to setup everything. Thanks to all the great help I found in the Internet [ referenced as links ].\nUbuntu Drivers: At first glance, it was nice and clean. I really the ubuntu drivers, may be it was in the ubuntu 22.x or 20.04.\n1 sudo ubuntu-drivers list 1 2 3 4 5 6 7 8 9 10 11 nvidia-driver-520, (kernel modules provided by nvidia-dkms-520) nvidia-driver-470-server, (kernel modules provided by linux-modules-nvidia-470-server-generic-hwe-24.04) nvidia-driver-555, (kernel modules provided by nvidia-dkms-555) nvidia-driver-470, (kernel modules provided by linux-modules-nvidia-470-generic-hwe-24.04) nvidia-driver-535-server, (kernel modules provided by linux-modules-nvidia-535-server-generic-hwe-24.04) nvidia-driver-515, (kernel modules provided by nvidia-dkms-515) nvidia-driver-550, (kernel modules provided by linux-modules-nvidia-550-generic-hwe-24.04) nvidia-driver-525, (kernel modules provided by nvidia-dkms-525) nvidia-driver-535, (kernel modules provided by linux-modules-nvidia-535-generic-hwe-24.04) nvidia-driver-545, (kernel modules provided by nvidia-dkms-545) nvidia-driver-560, (kernel modules provided by nvidia-dkms-560) \u0026lt; The result might be different for the fresh out of the box Ubuntu \u0026gt;\nand the best advertised part was :\n1 2 sudo ubuntu-drivers autoinstall # It will install the best latest version for your device. haha But, it is great to have all the nvidia-drivers installed right after the fresh OS update. cherry on top, NVIDIA-SMI works. Now, I only need to install libraries like cuda and cudnn for AI/ML pipeline.\nInstalling Nvidia CUDA Before Installing the CUDA, check the compalibility table. Nvida CUDA Compability and Table\nso, I chose (CUDA 12.2) as the nvidia driver 535.54.03+ was already installed on. Now to install the CUDA, Nvidia gave you three options based on the Architecture and Distribution selected: Nvidia CUDA installation\nBut wait, there is no ubuntu 24.04 version. So, You can\u0026rsquo;t run the network options or runfile. You have to rely on the 22.04 version deb local method.\nso I followed the following instruction method. Note: I had added the -12-2, otherwise it was installing the latest CUDA, which was 12.6\n1 2 3 4 5 6 7 8 9 sudo apt-get install linux-headers-$(uname -r) sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2204-12-2-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda-12-2 Here, the problem starts, it won\u0026rsquo;t install the cuda as it requires Nvidia-driver 560.x, and you have 535.x.\nInstalling latest or any Nvidia Driver version in Ubuntu [RabbitHole] At this point, I thought lets install Nvidia-driver 560.x. I found a really well documented guide: If Not Trye Then False\u0026rsquo;s Guide for Install Nvidia Driver\nThis thing is great, but I have question for you. Do you know Nvidia Optimus ? what is Initramfs / prime-select / run level 3 / gdm3 / lightdm / DKMS / MODPROBE / Wayland / X org ?\nif not, dont go in the rabbithole as you might end up having problem with it. if you want to learn about it : Nvidia Optimus: a comfy guide Luckily the above guide have section : if you see Black Screen after Nvidia drivers install, then check following video\nManuall DPKG Nvidia Driver installation Eventually, the better way to install the CUDA was to dont trust the Ubuntu for the Nvidia Drivers. Based on the great post\n1 2 sudo dpkg --force-all -P nvidia-firmware-535-535.54.03 nvidia-kernel-common-535 nvidia-compute-utils-535 sudo apt --fix-broken install Yes, you break the system and then fix the broken. Luckily, there was no black screen when I was doing this. And I am also\ncheck Nvidia-smi, you should have Nvidia 535.x installed.\nNow you could run the above code:\n1 2 3 sudo dpkg -i cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb sudo apt update sudo apt install cuda solving CUDA dependencies: libtinfo5 if you have problem with libtinfo5, Based on the site: libtinfo5\n1 2 3 4 5 6 7 8 9 sudo apt install software-properties-common curl -O http://launchpadlibrarian.net/648013231/libtinfo5_6.4-2_amd64.deb curl -O http://launchpadlibrarian.net/648013227/libncurses5_6.4-2_amd64.deb sudo dpkg -i libtinfo5_6.4-2_amd64.deb sudo dpkg -i libncurses5_6.4-2_amd64.deb sudo apt install libnss3-tools curl -O http://launchpadlibrarian.net/646633572/libaio1_0.3.113-4_amd64.deb sudo dpkg -i libaio1_0.3.113-4_amd64.deb sudo apt install cuda-12-2 It should run and add this to ~/.profile\n1 2 3 4 5 # set PATH for cuda 12.2 installation if [ -d \u0026#34;/usr/local/cuda-12.2/bin/\u0026#34; ]; then export PATH=/usr/local/cuda-12.2/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} fi That should give you nvcc version\n1 2 . ~/.profile nvcc --version 1 2 3 4 5 nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2023 NVIDIA Corporation Built on Tue_Aug_15_22:02:13_PDT_2023 Cuda compilation tools, release 12.2, V12.2.140 Build cuda_12.2.r12.2/compiler.33191640_0 You can install cuDNN if you are into training Neural Network, check the compability version of cuDNN again: compability matrix\n1 sudo apt install cudnn9 And thats all you need.\nBonus: 1. Installing Pyenv in the Fresh Ubuntu 24.04 Yes, in Ubuntu right out of the box, Pyenv installation won\u0026rsquo;t work. You need to add following Prerequists:\n1 sudo apt install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev then\n1 2 3 4 5 6 7 curl https://pyenv.run | bash echo \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc^Jecho \u0026#39;[[ -d $PYENV_ROOT/bin ]] \u0026amp;\u0026amp; export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc^Jecho \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc exec zsh sudo apt install python3-pip pyenv install 3.13 pyenv install 3.12 pyenv install 3.11 and so on ..\n2. Installing Virtualenv in the Fresh Ubuntu 24.04 Pipenv\n1 2 3 4 sudo apt install software-properties-common python-software-properties sudo add-apt-repository ppa:pypa/ppa sudo apt update sudo apt install pipenv Poetry:\n1 curl -sSL https://install.python-poetry.org | python3 - 3. Checking Cuda with Pytorch Script. Here is a small script to test the GPU and the packages. You need to install Torch, Torchvision , Numpy to run the script. You can use the latest packages for this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import torchvision import torch import traceback tcuda: torch.cuda = torch.cuda print(f\u0026#34;How many gpu in the server ? -\u0026gt; {tcuda.device_count()}\u0026#34;) print(f\u0026#34;can torch access {tcuda.device_count()} GPUs? :\u0026gt; {tcuda.is_available()}\u0026#34;) print(f\u0026#34;Attempting to access GPU device name : {tcuda.current_device()} ....\u0026#34;) try: model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True) model.cuda() model.eval() x = [torch.rand(3, 300, 400).cuda(), torch.rand(3, 500, 400).cuda()] predictions = model(x) print(predictions) print(f\u0026#34;is torch gpu is initialized ? :\u0026gt; {tcuda.is_initialized()}\u0026#34;) print(f\u0026#34;torch cuda memory summary :\u0026gt; {tcuda.memory_summary()}\u0026#34;) print(f\u0026#34;memory reserved {tcuda.memory_reserved()}\u0026#34;) print(f\u0026#34;memory allocated {tcuda.memory_allocated()}\u0026#34;) except Exception as e: print(e) print(\u0026#34;No CUDA available\u0026#34;) traceback.print_exc() 4. setting up the Terminal ZSH Install Some cool Nerd Fonts\nInstall zsh and oh my zsh and Terminator\n1 2 3 sudo apt install zsh sudo apt install terminator sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Install Oh my zsh plugins\n1 2 3 4 git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting git clone https://github.com/zdharma-continuum/fast-syntax-highlighting.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/plugins/fast-syntax-highlighting git clone --depth 1 -- https://github.com/marlonrichert/zsh-autocomplete.git $ZSH_CUSTOM/plugins/zsh-autocomplete Install Powerlevel10k\n1 git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k Update this in the ~/.zshrc\n1 2 ZSH_THEME=\u0026#34;powerlevel10k/powerlevel10k\u0026#34; plugins=(git zsh-autosuggestions zsh-syntax-highlighting fast-syntax-highlighting zsh-autocomplete python) and update the zshrc and setup the theme\n1 2 3 nano ~/.zshrc exec zsh p10k configure Atuin 1 2 3 4 5 curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -LsSf https://setup.atuin.sh | sh atuin login source $HOME/.atuin/bin/env atuin login atuin sync Handle SSH I prefer to run multiple ssh key in a single machine and it is handled by a ssh config file.\ncat config\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #Default GitHub Host github.com HostName github.com User git IdentityFile ~/.ssh/id_rsa Host github-mac HostName github.com User git IdentityFile ~/.ssh/id_rsa_personal Host gitlab HostName gilab.com IdentityFile ~/.ssh/id_rsa_personal id_rsa could be your one key and id_rsa_personal could be your personal / other github user.\n1 2 3 4 5 cd ./ssh sudo cp /home/shekhar/Downloads/ssh/. . ssh -T git@github.com ssh-add id_rsa ssh-add id_rsa_personal ","permalink":"https://shekharkoirala.github.io/posts/nvidia_cuda12_ubuntu/","summary":"MOTIVE So, Ubuntu 24.04 LTS was released on 25 April 2024. Its been more 3 months from the release and I thought it would be safe enough to install it in my main working computer. Wrong!! I wanted a documentation for setting up the CUDA 12.2 in Ubuntu 24.04 but ended for reinstalling everything. Thus, here is the blog for future me to setup everything. Thanks to all the great help I found in the Internet [ referenced as links ].","title":"Install Cuda 12.2 in Ubuntu 24.04"},{"content":"Background In this case study, Artificial Intelligence based system is used to take buisness decisions. The study is based on a e-commerce company which ingest lots of data like images, quarterly reports, invoices, customer feedbacks, customer reviews with images, etc. It would be great if this large amount of data is automatically categorized or grouped together. Previously, a machine learning team in the company had created a model to classify the input data. But the nature of the e-commerce data, new product launches, new customer feedbacks, etc. are not predictable. Thus, the model needs to be retrained frequently. The company is looking for a solution that can automatically perform and help the analyst to get the insight from the unstructured data.\nImage Clustering Lets dive in the Technical side of the case study. There are multiple paradigms of supervised machine learning, such as classification, object detection, and segmentation, among others. There are many robust supervised algorithms; however, to achieve high accuracy, we need labeled data, which is costly and time-consuming. Clustering is an unsupervised learning technique that can be applied to unlabeled datasets. It is one of the most interesting and challenging problems in the field of computer vision. The goal of image clustering is to group similar images together in the same cluster. There are also other forms of machine learning, such as semi-supervised learning, reinforcement learning, and transfer learning, which are not covered in this article. The main goal of the article is to explore the image clustering technique beyond its traditional use cases.\nImage clustering: View from Professional settings In real world scenario, a company could ingest images from multiple sources. The image sizes / format , interest point in the image are unknown. To fetch the insight from the unstructure settings is really hard and requires alot of regular hand picked tuning. Thus, Many companies moves towards supervised learning where they sample the data and label them and train a supervised model to get the insight.\nHowever, the unsupervised learning is also a good choice when the data is unpredictable and the labels are not available. The image clustering is one of the unsupervised learning technique which can be used to cluster the images based on the similarity and thus using this in the automated workflow to get the insight from the unstructured data.\nMethodology Image clustering systems can be divided into two main categories:\nImage feature Generation Image pixels are stored as matrics in the computer. The size of image matrics varies based on the image size and format. Thus Image pixel couldnot be used directly to custering algorithm due to its size and variability. Image features can be generated using various methods like neural networks, computer vision algorithms like SIFT, SURF, ORB, etc. Sometimes feature reduction techniques like PCA, LDA, etc are used to reduce the dimensionality of the features. Feature clustering After a feature is generated, the clustering algorithm is applied to the features to cluster the images. The clustering algorithm can be K-means, DBSCAN, Hierarchical clustering, etc. for the sake of this study, we are using a small dataset of brand logos. However in the study properitery e-commerce data [ logos from pdfs/ document ] are used. Note: It really depends on the company data, sometimes the whole image are used to generate the features and sometimes the interest points are used to generate the features. Segment Anything model can be used to generate the interest points. Here, an invoice is parsed by the model when the text prompt is logo and the bounding box over the Tesla logo is generated.\nModel Web Demo: Segment Anything\nImage Feature Generation The latest trend of Attention based models outperforms the traditional computer vision algorithms. The attention based models like ViT, ResNet, EfficientNet, etc are used to generate the features. Likewise, there are multi model models like CLIP, DALL-E, etc which are used to generate the features. Using the multi model models, the features can be used to compared with the possible text data and thus can be used to generate the insight from the images.\nfor the example of the image clustering, we will be using the openAI\u0026rsquo;s CLIP model to generate the features.\n1 2 3 4 from transformers import AutoProcessor, AutoTokenizer, CLIPModel model = CLIPModel.from_pretrained(\u0026#34;openai/clip-vit-large-patch14\u0026#34;) processor = AutoProcessor.from_pretrained(\u0026#34;openai/clip-vit-large-patch14\u0026#34;) a good alternative to the CLIP model is the VIT model.\n1 2 3 4 from transformers import ViTImageProcessor, ViTForImageClassification processor = ViTImageProcessor.from_pretrained(\u0026#39;google/vit-base-patch16-224\u0026#39;) model = ViTForImageClassification.from_pretrained(\u0026#39;google/vit-base-patch16-224\u0026#39;) Resource to browse the models for feature generation: Huggingface\nOnce the model is loaded, we can generate the features using the model.\n1 2 3 with torch.no_grad(): inputs = processor(images=batch_images, return_tensors=\u0026#34;pt\u0026#34;) image_feature_data = model.get_image_features(**inputs) It is important that the processor should be feed in the batch image data. since the the underlying model is trained on the batch image data. Thus batch image helps to achieve better throughput for generating the features.\nImage clustering Algorithm Once the features are generated, the clustering algorithm is applied to the features. The clustering algorithm can be K-means, DBSCAN, Hierarchical clustering, etc. Here, we choose Faiss for the clustering as it has both CPU and GPU support.\n1 2 3 4 5 6 7 8 import faiss n_centroids = 5 # number of clusters n_iter = 100 # number of iterations verbose = True feature_data_dim = feature_data.shape[1] # dimension of the feature data kmeans = faiss.Kmeans(feature_data_dim, ncentroids, niter=niter, verbose=verbose) kmeans.train(feature_data) Lets see How it creates the cluster. The image clustering result doesnot make sense. lets update the hyperparamter and create the clustering again with different number of clusters.\n1 n_centroids = 35 # number of clusters The clustering result is much better. The cluster result is better than the previous as we increased the number of clusters. The number of clusters can be determined based on the domain knowledge of the dataset or using the metrics.\nImage clustering Metrics Finding the best cluster size is a challengine task. The perfect cluster size can be predicted if some domain knowledge of the dataset is available. In the absence of domain knowledge, we can use the metrics to find the best cluster size.\nImage clustering can be compared based on two types of metrics.\nExtrinsic metrics: Extrinsic metrics are based on the ground truth labels. Some of the popular extrinsic metrics are: Adjusted Rand Index (ARI) Normalized Mutual Information (NMI) Fowlkes-Mallows Index (FMI) Homogeneity, Completeness, and V-measure Precision, Recall, and F1-score Confusion matrix However, we are into the unsupervised learning, we will not be using these metrics.\nIntrinsic metrics: Intrinsic metrics are based on the clustering results. Some of the popular intrinsic metrics are: Silhouette score Davies-Bouldin Index Calinski-Harabasz Index Gap statistic Elbow method Lets see how we can use these metrics to evaluate the clustering results.\nelbow method is used to find the best cluster size.\nThe elbow method is used to find the best cluster size. The best cluster size is the point where the curve starts to flatten. In the above image, the best cluster size is 35/40.\nSimilarly, other metrics can be used to evaluate the clustering results.\nThe blog only highlights the specfic part of the image clustering system. All the necessary code snippets are provided to get started with the image clustering. Many software enhancement and glue code are skipped for the sake of the article.\nResult A image clustering system is created and implemented in the system and the data is forwarded to the analyst for the further analysis. The system is able to cluster the data based on the cluster and thus necessary action can be taken much faster. As the system is automated, many documents gets labelled eventually and thus labelling the cluster is much easier than labelling the whole dataset.\n","permalink":"https://shekharkoirala.github.io/posts/image_clustering_intro/","summary":"Background In this case study, Artificial Intelligence based system is used to take buisness decisions. The study is based on a e-commerce company which ingest lots of data like images, quarterly reports, invoices, customer feedbacks, customer reviews with images, etc. It would be great if this large amount of data is automatically categorized or grouped together. Previously, a machine learning team in the company had created a model to classify the input data.","title":"Case study: Automated Image Clustering for E-commerce company"},{"content":"","permalink":"https://shekharkoirala.github.io/posts/data_sourcing_ml/","summary":"","title":"Data_sourcing_ml"},{"content":"This is based on a workshop on Pycon Ireland 2023 given by : Pavlo Tishkin\nSetup AWS account Setup AWS CLI Setup Terraform Setup Python Setup AWS credentials Setup AWS S3 bucket Setup AWS Lambda function Setup AWS CloudWatch event Setup AWS IAM role Setup AWS IAM policy setup lamda function \u0026laquo; updating soon\u0026raquo;\n","permalink":"https://shekharkoirala.github.io/posts/terraform_aws_python/","summary":"This is based on a workshop on Pycon Ireland 2023 given by : Pavlo Tishkin\nSetup AWS account Setup AWS CLI Setup Terraform Setup Python Setup AWS credentials Setup AWS S3 bucket Setup AWS Lambda function Setup AWS CloudWatch event Setup AWS IAM role Setup AWS IAM policy setup lamda function \u0026laquo; updating soon\u0026raquo;","title":"Terraform_aws_python"},{"content":"How to install luminance HDR in ubuntu install dependecies 1 sudo apt install -y qtcreator qtbase5-dev qt5-qmake cmake libexiv2-dev libtiff-dev libraw-dev libpng-dev libjpeg-dev libopenexr-dev libfftw3-dev libboost-all-dev libcfitsio-dev libgsl-dev This command installs the following dependencies:\nQT5 development tools exiv2 library libtiff library libraw library libpng library libjpeg library libopenexr library fftw3 library libboost library cfitsio library Gnu Gsl library sudo apt install qttools5-dev sudo apt-get install qttools5-dev-tools sudo apt install libqt5webkit5-dev sudo apt-get install libqt5svg5-dev sudo apt-get install libeigen3-dev\ninstall luminance hdr\n1 2 3 4 5 6 7 8 9 10 11 12 git clone cd luminance-hdr mkdir build cd build cmake \\ 5s 12:07:11 PM -DCMAKE_BUILD_TYPE=\u0026#34;Release\u0026#34; \\ -DCMAKE_INSTALL_PREFIX=\u0026#34;$HOME/programs/lhdr\u0026#34; \\ .. make sudo make install ","permalink":"https://shekharkoirala.github.io/posts/install_luminance_ubuntu/","summary":"How to install luminance HDR in ubuntu install dependecies 1 sudo apt install -y qtcreator qtbase5-dev qt5-qmake cmake libexiv2-dev libtiff-dev libraw-dev libpng-dev libjpeg-dev libopenexr-dev libfftw3-dev libboost-all-dev libcfitsio-dev libgsl-dev This command installs the following dependencies:\nQT5 development tools exiv2 library libtiff library libraw library libpng library libjpeg library libopenexr library fftw3 library libboost library cfitsio library Gnu Gsl library sudo apt install qttools5-dev sudo apt-get install qttools5-dev-tools sudo apt install libqt5webkit5-dev sudo apt-get install libqt5svg5-dev sudo apt-get install libeigen3-dev","title":"Install_luminance_ubuntu"},{"content":"How to use your XT1/2/3/4 fujifilm camera as webcam for your ubuntu 22.04. This is a simple guide to use your fujifilm camera as a webcam.\nInstallation: 1 sudo apt-get install gphoto2 v4l2loopback-utils v4l2loopback-dkms ffmpeg modprobe setup\n1 2 3 sudo modprobe -r v4l2loopback sudo modprobe v4l2loopback exclusive_caps=1 max_buffers=2 check video sources\n1 ls -l /dev/video* Make sure there is no gphoto process already running:\n1 ps aux | grep gphoto check is camera is detected by gphoto2:\n1 gphoto2 --auto-detect start the webcam\n1 gphoto2 --stdout --capture-movie | ffmpeg -i - -vcodec rawvideo -pix_fmt yuv420p -threads 0 -f v4l2 /dev/video2 Note: Huge thanks to : https://medium.com/nerdery/dslr-webcam-setup-for-linux-9b6d1b79ae22\n","permalink":"https://shekharkoirala.github.io/posts/fujifilm_as_webcam/","summary":"How to use your XT1/2/3/4 fujifilm camera as webcam for your ubuntu 22.04. This is a simple guide to use your fujifilm camera as a webcam.\nInstallation: 1 sudo apt-get install gphoto2 v4l2loopback-utils v4l2loopback-dkms ffmpeg modprobe setup\n1 2 3 sudo modprobe -r v4l2loopback sudo modprobe v4l2loopback exclusive_caps=1 max_buffers=2 check video sources\n1 ls -l /dev/video* Make sure there is no gphoto process already running:\n1 ps aux | grep gphoto check is camera is detected by gphoto2:","title":"Fujifilm_as_webcam"},{"content":"ffmpeg Recipes A list of useful receipes when using ffmpeg. A dump for future references.\nExtract frames from video:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/zsh # Find all .mp4 files in the current directory and store them in an array input_files=(*.mp4) # Loop through each input file for input_file in \u0026#34;${input_files[@]}\u0026#34; do # Get the path to the input file directory input_dir=\u0026#34;$(dirname \u0026#34;$input_file\u0026#34;)\u0026#34; # Create a folder for the input file images in the same directory as the input file folder_name=\u0026#34;${input_file%.*}_images\u0026#34; output_dir=\u0026#34;$input_dir/$folder_name\u0026#34; mkdir -p \u0026#34;$output_dir\u0026#34; # Extract images from the input file and save them to the folder ffmpeg -i \u0026#34;$input_file\u0026#34; -r 1 \u0026#34;$output_dir/output_%03d.png\u0026#34; done combine video.mp4 and audio.mp4 to a single video.\n1 ffmpeg -i video.mp4 -i audio.mp4 -c copy -map 0:0 -map 1:1 -shortest combined.mp4 convert .h265 video to .h264\n1 ffmpeg -i video1.mov -vcodec h264 -acodec mp2 video1.mp4 ","permalink":"https://shekharkoirala.github.io/posts/fury_ffmpeg/","summary":"ffmpeg Recipes A list of useful receipes when using ffmpeg. A dump for future references.\nExtract frames from video:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/zsh # Find all .mp4 files in the current directory and store them in an array input_files=(*.mp4) # Loop through each input file for input_file in \u0026#34;${input_files[@]}\u0026#34; do # Get the path to the input file directory input_dir=\u0026#34;$(dirname \u0026#34;$input_file\u0026#34;)\u0026#34; # Create a folder for the input file images in the same directory as the input file folder_name=\u0026#34;${input_file%.","title":"Fast Fury and Fantastic .... The ffmpeg"},{"content":"Introduction The post is a quick guide on \u0026ldquo;How we could use AWS lambda services to deploy some standalone task\u0026rdquo;. This is really helpful when we dont have to buy instances for such task. By using word task, I mean scripts, that could scrap data, or a cron job data aggregator. The BEST thing about the cron job is that , it is way cheaper. A million hits by lambda function is Free in AWS.\nThe following steps could lets you get going. In this example below, I am aggregating data in MongoDB and store the output in Redis.\nStep 1 function creating Create function : https://console.aws.amazon.com/lambda/home?region=us-east-1 One has to login to the AWS console and create a lambda function. Think Lambda function as a task that we want to do here.\nTo run a function , we need a script. Lets say : lambda_function.py The script wont run until it mets all its dependency. In Data Science world, pandas , numpy etc. We need to provide all the Dependency a script it requires.\nStep 2 Dependencies For managing the Dependencies : we could looked at the official Documentation. https://docs.aws.amazon.com/lambda/latest/dg/python-package.html In short: We want to dump all the dependencies in a folder.\n1 2 3 pip install --target ./package redis pip install --target ./package sentry_sdk pip install --target ./package motor Yeah, I didnt include pandas, numpy and scipy. Because that is different case. The AWS lambda works in different version of linux which doesnt support the pypi installation of pandas and numpy. As those libraries extensibly use cython for optimization.\nTo use such Libraries, we will get to the new concept of Lmbda function ie : LAYERS\nstep 3: the code Make your code ready, especially the Beginning and the end. Most of the standard python use config.ini or .env file to get the variables value. Change that to os.environ.gte() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import motor.motor_asyncio import asyncio from datetime import datetime, timedelta import os import pandas as pd import sentry_sdk import redis import json class TrendingAggregator: def __init__(self): sentry_url = os.environ.get(\u0026#34;default\u0026#34;, \u0026#34;sentry_url\u0026#34;) sentry_env = os.environ.get(\u0026#34;default\u0026#34;, \u0026#34;sentry_env\u0026#34;) Similarly, at the end of the code.\n1 2 3 def lambda_handler(event, context): trending = TrendingAggregator() trending.process() You could change the function name lambda_handler and the filename lambda_function.py but it requires you to change the name in AWS lambda function console too. So, for sake of lazyness, lets stick to the default naming.\nStep 4: shipment Make the shipment ready.\n1 2 3 4 5 ~/my-function$ cd package ~/my-function/package$ zip -r9 ${OLDPWD}/function.zip . $ cd .. $ zip -g function.zip lambda_function.py Here, what we did is , we zipped the packages [ dependencies ] and then add our script to the function.zip file.\nStep 5: layers ? You could see Function code, when you scroll down in the lambda dunction page in your AWS console , and Actions box, where one get options like upload a .zip file or upload a file from Amazon S3 Once you upload your zip file. You will see your code [ unless your code is more that 10 MB ]\nThere you could save and test the code. But since you need more complex dependencies You need to know little mroe about layers.\nStep 5 : yeah, the layers. Layers: If your code depends on AWS lambda function layers, you could do Three things to solve this. link to add layer to your lambda function https://console.aws.amazon.com/lambda/home?region=us-east-1#/add/layer?function=trending_aggregator Note: trending-aggregator is my function name.\nuse standard layer more info https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/ its easy, its provided by AWS ( safe )\nOne could select Select from list of runtime compatible layers when they are in their layer page and see the AWS standard layer name. AWSLambda-Python6-SciPy1x\nuse someone else layer yeah, well, whatever you gonna do, someone had already done it may be 5 years ago. So, you could use their layer. Until you have their layer code. Luckily someone shares the detail in a stackoverfflow question. Cheers. more info https://stackoverflow.com/questions/36054976/pandas-aws-lambda One could select Provide a layer version ARN in their layer page. and add this arn code arn:aws:lambda:us-east-1:113088814899:layer:Klayers-python37-pandas:1 BOOM , Thanks Mate.\nuse your own layer Security Issues ?, To create our own layer , we need another AWS machine ( EC2). Login to another EC2 machine and follow these steps:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 python --version Python 3.6.8 # https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html # python 3.6 uses Amazon Linux currently mkdir project cd project virtualenv v-env source ./v-env/bin/activate pip install pandas deactivate # creating layer # https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html#configuration-layers-path mkdir python cd python cp -r ../v-env/lib/python3.6/site-packages/* . cd .. zip -r panda_layer.zip python This will create a zip file which include pandas , as installed natively in a weird AWS machine.\n1 2 3 4 5 6 7 8 9 ubuntu@ip:~/project$ cd ~/.aws/ ubuntu@ip:~/.aws$ ls config credentials credentials_prod ubuntu@ip:~/.aws$ cp credentials credentials_dev ubuntu@ip:~/.aws$ aws configure AWS Access Key ID [****************VF5Q]: *********AWS Id ************ AWS Secret Access Key [****************hco4]: *********AWS access key ************ Default region name [us-east-1]: Default output format [json]: Here, we make our AWS credential compatible. Now we have a zip containing dependency and our AWS account is connected. As suggested by one stackoverflow answer.\n1 2 aws lambda publish-layer-version --layer-name pandas --zip-file fileb://panda_layer.zip --compatible-runtimes python3.6 But it didnt, so Our DevOps engineer finds a hack to deploy this as a layer. We first push the zip file to a S3 bucket and then publish the layer from there.\n1 2 aws s3 cp layer.zip s3://{{bucket_name}}/layer.zip aws lambda publish-layer-version --layer-name my-layer --description \u0026#34;My layer\u0026#34; --license-info \u0026#34;MIT\u0026#34; --content S3Bucket={{bucket-name}},S3Key=layer.zip --compatible-runtimes python3.6 python3.7 python3.8 yeah, owning one own layer is hard, But we already have two other alternatives.\nStep 6 RUNNNN Run, you could test the model, some database dependency like redis and mongodb should be abale to communicate with AWS lambda otherwise it wont work. But Once you do that, it will surely work and you will see the bueaty.\n1 REPORT RequestId: 5b9ce656-c38c-43c8-9187-515872598c61\tDuration: 3003.21 ms\tBilled Duration: 3000 ms\tMemory Size: 192 MB\tMax Memory Used: 142 MB\tInit Duration: 2202.47 ms\tYou will only get billed for the time you ran for. You could limit the resources, see Memory size and Max Memory used.\nI hope the stats are convincing enough to adopt Lambda function instead a buying bulky Instances.\n","permalink":"https://shekharkoirala.github.io/posts/lambda_function_python/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe post is a quick guide on \u0026ldquo;How we could use AWS lambda services to deploy some standalone task\u0026rdquo;. This is really helpful when we dont have to buy instances for such task. By using word task, I mean scripts, that could scrap data, or a cron job data aggregator. The BEST thing about the cron job is that , it is way cheaper. A million hits by lambda function is Free in AWS.\u003c/p\u003e","title":"AWS Lambda function for Data Scientist using Python."},{"content":"Introduction I want to create a Blog. But there are some conditions that I want:\nEasy/simple to create The UI should match my style. Based on Git, for future reference. I chosed Hugo. because it is based on MarkDown.\nWhy MarkDown: Markdown is simple to learn, with minimal extra characters, so itâ€™s also quicker to write content. Less chance of errors when writing in Markdown. Produces valid XHTML output. Keeps the content and the visual display separate, so you cannot mess up the look of your site. Write in any text editor or Markdown application you like. Markdown is a joy to use Why Hugo: Hugo is created based on go. A respected and fast programming language. So, the technology stack is nice. beside that, it is based on Markdown. There are few alternatives of Hugo. Like pelican, based on Python: No hard feelings.\nI found a lots of prebuilt themes, which make the boring task easy.\nand you know what , when I know what hugo can do. I need\nproper code formatting in the blog proper user commenting in each posts google analytics animation stuff tags categories search \u0026mdash;\u0026mdash; blah blah , and I dont need write any html and css. That\u0026rsquo;s awesome.\nInstallation Lets start the work.\nInstall Hugo: For other OS : Install Hugo\n1 brew install hugo create space: The naming of folder all yours\n1 2 hugo new site personalBlog cd personalBlog Add Theme Luckily, hugo has huge collection of themes. Mine one is based on theme: loveit 1 2 git init git submodule add https://github.com/dillonzq/LoveIt.git themes/loveit Add a new post 1 hugo new posts/my_post.md yeah , its that easy.\nConfiguration To start the server. lets fix a-little default config.\nopen config.toml in one of the text editor and add this configs there.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 baseURL = \u0026#34;http://example.org/\u0026#34; # [en, zh-cn, fr, ...] determines default content language defaultContentLanguage = \u0026#34;en\u0026#34; # language code languageCode = \u0026#34;en\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; # Change the default theme to be use when building the site with Hugo theme = \u0026#34;LoveIt\u0026#34; [params] # LoveIt theme version version = \u0026#34;0.2.X\u0026#34; [menu] [[menu.main]] identifier = \u0026#34;posts\u0026#34; # you can add extra information before the name (HTML format is supported), such as icons pre = \u0026#34;\u0026#34; # you can add extra information after the name (HTML format is supported), such as icons post = \u0026#34;\u0026#34; name = \u0026#34;Posts\u0026#34; url = \u0026#34;/posts/\u0026#34; # title will be shown when you hover on this menu link title = \u0026#34;\u0026#34; weight = 1 [[menu.main]] identifier = \u0026#34;tags\u0026#34; pre = \u0026#34;\u0026#34; post = \u0026#34;\u0026#34; name = \u0026#34;Tags\u0026#34; url = \u0026#34;/tags/\u0026#34; title = \u0026#34;\u0026#34; weight = 2 [[menu.main]] identifier = \u0026#34;categories\u0026#34; pre = \u0026#34;\u0026#34; post = \u0026#34;\u0026#34; name = \u0026#34;Categories\u0026#34; url = \u0026#34;/categories/\u0026#34; title = \u0026#34;\u0026#34; weight = 3 # Markup related configuration in Hugo [markup] # Syntax Highlighting (https://gohugo.io/content-management/syntax-highlighting) [markup.highlight] # false is a necessary configuration (https://github.com/dillonzq/LoveIt/issues/158) noClasses = false Now lets server the site , and check in localhost:1313\n1 hugo serve Preview Does it look like this ? May be you ask where is the profile in the homepage ? In order to enable that\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # Home page config [params.home] # LoveIt NEW | 0.2.0 amount of RSS pages rss = 10 # Home page profile [params.home.profile] enable = true # Gravatar Email for preferred avatar in home page gravatarEmail = \u0026#34;\u0026#34; # URL of avatar shown in home page avatarURL = \u0026#34;/images/avatar.png\u0026#34; # LoveIt CHANGED | 0.2.7 title shown in home page (HTML format is supported) title = \u0026#34;\u0026#34; # subtitle shown in home page subtitle = \u0026#34;Personal Blog / Scribbles\u0026#34; # whether to use typeit animation for subtitle typeit = true # whether to show social links social = true # LoveIt NEW | 0.2.0 disclaimer (HTML format is supported) disclaimer = \u0026#34;\u0026#34; # Home page posts [params.home.posts] enable = true # special amount of posts in each home posts page paginate = 6 # LoveIt DELETED | 0.2.0 replaced with hiddenFromHomePage in params.page # default behavior when you don\u0026#39;t set \u0026#34;hiddenFromHomePage\u0026#34; in front matter defaultHiddenFromHomePage = false More configs can be found here: site-configurations\nDeployment Now lets share what we have created here. But to do that we need to create static site from this markdown. Because only static site is get hosted on the github. No worries, all handled by Hugo.\nCreate a repo in the github based on your \u0026lt;username\u0026gt;/github.io and add it as submodule in the folder.\n1 git submodule add -b master https://github.com/\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git public As, suggested by our mighty Hugo, create a deploy.sh file and it here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/sh # If a command fails then the deploy stops set -e printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; # Build the project. hugo -t loveit # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site $(date)\u0026#34; if [ -n \u0026#34;$*\u0026#34; ]; then msg=\u0026#34;$*\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master Basically, it generate static site push the code to github. make sure everything working.Now make the script executable.\n1 chmod +x deploy.sh 1 ./deploy.sh \u0026#34;Your optional commit message\u0026#34; Done \u0026hellip; !!! checkthe site at \u0026lt;username\u0026gt;.github.io\nFuture We have created a site , but we are not satisfied yet. yeah i know.\ngoogle analytics : Done Facebook Comments : Done Agolia Search : Done Add Jupyter notebooks in the blog. Using custom domain for the site. I will look into this topic pretty soon and update in the post.\nReferences Hugo Install Hugo Quickstart Hugo Host Hugo in github Proper summaries of Blog Hugo themes loveit main site Loveit loveit Docs Real world inpiring Example: loveit markdown markdown syntax Add comments to support this Blog.\n","permalink":"https://shekharkoirala.github.io/posts/first_post/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI want to create a Blog. But there are some conditions that I want:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEasy/simple to create\u003c/li\u003e\n\u003cli\u003eThe UI should match my style.\u003c/li\u003e\n\u003cli\u003eBased on Git, for future reference.\u003c/li\u003e\n\u003c/ol\u003e","title":"Create your own Blog Post"},{"content":"Work Machine Learning Engineer : Identv Software Engineer : Ekbana\nEducation Online Masters in Computer Science - Georgia Tech Electronics and Communication Engineer - IOE, Thapathali Campus\n","permalink":"https://shekharkoirala.github.io/about/","summary":"Work Machine Learning Engineer : Identv Software Engineer : Ekbana\nEducation Online Masters in Computer Science - Georgia Tech Electronics and Communication Engineer - IOE, Thapathali Campus","title":"shekhar koirala"}]