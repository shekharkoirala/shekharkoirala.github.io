<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Setting up Nvidia SDK Manager and Torch Library in Jetson Board | shekhar's Blog</title>
<meta name=keywords content="Nvidia,CUDA,Nvidia SDK Manager,Jetson,pyenv,poetry,tensorrt,benchmark"><meta name=description content="Learn how to setup Nvidia SDK Manager to install latest jetpack in jetson development boards like Orion AGX, Nano Xavier. This blog also extent to setup tensorrt and do benchmark over existing Pytorch Model"><meta name=author content="shekhar"><link rel=canonical href=https://shekharkoirala.github.io/posts/nvidia_jetson_sdk_jetpack_setup/><meta name=google-site-verification content="UA-168093774-1"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://shekharkoirala.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shekharkoirala.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shekharkoirala.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://shekharkoirala.github.io/apple-touch-icon.png><link rel=mask-icon href=https://shekharkoirala.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://shekharkoirala.github.io/posts/nvidia_jetson_sdk_jetpack_setup/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://shekharkoirala.github.io/posts/nvidia_jetson_sdk_jetpack_setup/"><meta property="og:site_name" content="shekhar's Blog"><meta property="og:title" content="Setting up Nvidia SDK Manager and Torch Library in Jetson Board"><meta property="og:description" content="Learn how to setup Nvidia SDK Manager to install latest jetpack in jetson development boards like Orion AGX, Nano Xavier. This blog also extent to setup tensorrt and do benchmark over existing Pytorch Model"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-19T13:37:53+01:00"><meta property="article:modified_time" content="2024-08-19T13:37:53+01:00"><meta property="article:tag" content="Nvidia"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="Nvidia SDK Manager"><meta property="article:tag" content="Jetson"><meta property="article:tag" content="Pyenv"><meta property="article:tag" content="Poetry"><meta property="og:image" content="https://shekharkoirala.github.io/images/jetpack-nvidia-jetson.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shekharkoirala.github.io/images/jetpack-nvidia-jetson.jpeg"><meta name=twitter:title content="Setting up Nvidia SDK Manager and Torch Library in Jetson Board"><meta name=twitter:description content="Learn how to setup Nvidia SDK Manager to install latest jetpack in jetson development boards like Orion AGX, Nano Xavier. This blog also extent to setup tensorrt and do benchmark over existing Pytorch Model"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://shekharkoirala.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Setting up Nvidia SDK Manager and Torch Library in Jetson Board","item":"https://shekharkoirala.github.io/posts/nvidia_jetson_sdk_jetpack_setup/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Setting up Nvidia SDK Manager and Torch Library in Jetson Board","name":"Setting up Nvidia SDK Manager and Torch Library in Jetson Board","description":"Learn how to setup Nvidia SDK Manager to install latest jetpack in jetson development boards like Orion AGX, Nano Xavier. This blog also extent to setup tensorrt and do benchmark over existing Pytorch Model","keywords":["Nvidia","CUDA","Nvidia SDK Manager","Jetson","pyenv","poetry","tensorrt","benchmark"],"articleBody":"The blog serves as a backup for setting up the Nvidia Jetson Orion AGX for Development/Production. Please note: review all the steps before proceeding.\nI was updating from JetPack 5.x to 6.x. My Jetson Orion AGX had already been flashed.\nInstallation The blog covers installing the Nvidia sdk manager. Make sure to install Ubuntu 22.04, not 24.04, for installing JetPack 6 (as of August 19, 2024). Similarly, upgrade the host machine, install all the necessary Nvidia drivers and CUDA on the host machine before installing the Nvidia SDK Manager. More information on installing Nvidia Drivers on ubuntu\nPrerequistics It is also advisable to review the dedicated developer kit user guide Go over Jetson Zoo and check for package versions if needed: ZOO\nPrepare the Nvidia Jetson Device Once you log into the Nvidia SDK Manager, you can‚Äôt verify the target board. To do that, follow these steps:\nConnect the USB Type-C port located above the power jack to the power source. Connect the Type-C port located beside the GPIO pins to the host computer. Connect the Micro-USB [UART] port located beside the HDMI pins to the host computer. After this, make sure you boot the Nvidia Jetson in recovery mode. You can put the device in recovery mode in two ways:\nPress the middle button (between power and restart) and the restart button together, then release them together. [This step didn‚Äôt work for me.]\nTurn on the Nvidia Jetson machine, open the terminal, and type:\n1 sudo reboot --force forced-recovery After this the nvidia board will be seen in the Nvidia SDK Manager. You can also check it via command in host machine.\n1 lsusb Nvidia SDK Manager Once the target board is detected, select the drive location where you want to download the necessary files. Then, a prompt will appear to flash the board. Flash your board by selecting the eMMC drive. Once the flash process is completed, it will prompt you to proceed with the installation of the Nvidia SDK Manager. However, I encountered an issue where it wouldn‚Äôt allow me to install everything.\nHere‚Äôs a workaround: Open the terminal on the freshly flashed Nvidia board.\nConnect to Wi-Fi, because the flashed Nvidia board functions like a new computer. Install the necessary software and drivers to proceed.\n1 2 3 4 5 sudo apt update \u0026\u0026 sudo apt upgrade sudo apt dist-upgrade sudo apt install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev sudo apt install python3-pip sudo apt install nvidia-jetpack And now, voila! You have the latest version of Nvidia Jetson.\nInstalling python packages. Installing Python packages, including Poetry, can be tricky because TensorRT is installed in the system. To use it, you need to run the following command:\n1 poetry config virtualenvs.options.system-site-packages true Yes, TensorRT is included in JetPack. You can verify its installation using the following command:\n1 sudo apt-get install tensorrt nvidia-tensorrt-dev python3-libnvinfer-dev Next, install Torch, TorchVision, and ONNX Runtime.\n1 2 wget https://nvidia.box.com/shared/static/mp164asf3sceb570wvjsrezk1p4ftj8t.whl -O torch-2.3.0-cp310-cp310m-linux_aarch64.whl pip install torch-2.3.0-cp310-cp310m-linux_aarch64.whl for the wheel file url check this.\nforum posts , found both torch and torchvision, but might not update later. jetson zoo I didn‚Äôt torch here jetson distribution only found torch not torchvision. Yes, it‚Äôs a bit of a mess, but it works. We don‚Äôt have much flexibility with the Torch version‚Äîwe have to use whatever Nvidia provides for the JetPack versions.\nBenchmarking. Now, use the script to benchmark the speed of Torch and TensorRT.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 import torch import onnx import onnx_tensorrt.backend as backend import tensorrt as trt import time import numpy as np # Define a simple PyTorch model class MyModel(torch.nn.Module): def __init__(self): super().__init__() self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1) self.relu1 = torch.nn.ReLU() self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.relu2 = torch.nn.ReLU() self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = torch.nn.Linear(64 * 16 * 16, 512) self.relu3 = torch.nn.ReLU() self.fc2 = torch.nn.Linear(512, 10) def forward(self, x): x = self.conv1(x) x = self.relu1(x) x = self.conv2(x) x = self.relu2(x) x = self.pool(x) x = x.view(-1, 64 * 16 * 16) x = self.fc1(x) x = self.relu3(x) x = self.fc2(x) return x # Export the PyTorch model to ONNX model = MyModel() input_shape = (1, 3, 32, 32) input_names = ['input'] output_names = ['output'] dummy_input = torch.randn(input_shape) torch.onnx.export(model, dummy_input, 'my_model.onnx', verbose=False, input_names=input_names, output_names=output_names) # Load the ONNX model and create a TensorRT engine from it model_onnx = onnx.load('my_model.onnx') engine = backend.prepare(model_onnx, device='CUDA:0') # Create a context for executing inference on the engine context = engine.create_execution_context() # Allocate device memory for input and output buffers input_name = 'input' output_name = 'output' input_shape = (1, 3, 32, 32) output_shape = (1, 10) input_buf = trt.cuda.alloc_cuda_pinned_memory(trt.volume(input_shape) * trt.float32.itemsize) output_buf = trt.cuda.alloc_cuda_pinned_memory(trt.volume(output_shape) * trt.float32.itemsize) # Load the PyTorch model into memory and measure inference speed model.load_state_dict(torch.load('my_model.pth')) model.eval() num_iterations = 1000 total_time = 0.0 with torch.no_grad(): for i in range(num_iterations): start_time = time.time() input_data = torch.randn(input_shape) output_data = model(input_data) end_time = time.time() total_time += end_time - start_time pytorch_fps = num_iterations / total_time print(f\"PyTorch FPS: {pytorch_fps:.2f}\") # Create a TensorRT engine from the ONNX model and measure inference speed trt_engine = backend.prepare(model_onnx, device='CUDA:0') num_iterations = 1000 total_time = 0.0 with torch.no_grad(): for i in range(num_iterations): input_data = torch.randn(input_shape).cuda() start_time = time.time() output_data = trt_engine.run(input_data.cpu().numpy())[0] end_time = time.time() total_time += end_time - start_time tensorrt_fps = num_iterations /total_time tensorrt_fps = num_iterations / total_time print(f\"TensorRT FPS: {tensorrt_fps:.2f}\") print(f\"Speedup: {tensorrt_fps/pytorch_fps:.2f}x\") Output:\n1 2 3 4 (model-train-py3.10) jetson-dublin@ubuntu:~/Development/shekhar/model_train$ python benchmark.py PyTorch Inference Time: 0.007773 seconds TensorRT Inference Time: 0.002509 seconds TensorRT is 3.10x faster than PyTorch ","wordCount":"1003","inLanguage":"en","image":"https://shekharkoirala.github.io/images/jetpack-nvidia-jetson.jpeg","datePublished":"2024-08-19T13:37:53+01:00","dateModified":"2024-08-19T13:37:53+01:00","author":{"@type":"Person","name":"shekhar"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shekharkoirala.github.io/posts/nvidia_jetson_sdk_jetpack_setup/"},"publisher":{"@type":"Organization","name":"shekhar's Blog","logo":{"@type":"ImageObject","url":"https://shekharkoirala.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shekharkoirala.github.io/ accesskey=h title="shekhar's Blog (Alt + H)">shekhar's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://shekharkoirala.github.io/archives title="Post üìù"><span>Post üìù</span></a></li><li><a href=https://shekharkoirala.github.io/categories/ title="Categories üîñ"><span>Categories üîñ</span></a></li><li><a href=https://shekharkoirala.github.io/tags/ title="Tags üè∑Ô∏è"><span>Tags üè∑Ô∏è</span></a></li><li><a href=https://shekharkoirala.github.io/about title="About üßò"><span>About üßò</span></a></li><li><a href=https://shekharkoirala.github.io/search/ title=üîç><span>üîç</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://shekharkoirala.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://shekharkoirala.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Setting up Nvidia SDK Manager and Torch Library in Jetson Board</h1><div class=post-description>Learn how to setup Nvidia SDK Manager to install latest jetpack in jetson development boards like Orion AGX, Nano Xavier. This blog also extent to setup tensorrt and do benchmark over existing Pytorch Model</div><div class=post-meta><span title='2024-08-19 13:37:53 +0100 IST'>August 19, 2024</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;shekhar&nbsp;|&nbsp;<a href=mailto:shekharkoirala4@gmail.com rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#installation aria-label=Installation>Installation</a><ul><li><a href=#prerequistics aria-label=Prerequistics>Prerequistics</a></li><li><a href=#prepare-the-nvidia-jetson-device aria-label="Prepare the Nvidia Jetson Device">Prepare the Nvidia Jetson Device</a></li><li><a href=#nvidia-sdk-manager aria-label="Nvidia SDK Manager">Nvidia SDK Manager</a></li><li><a href=#installing-python-packages aria-label="Installing python packages.">Installing python packages.</a></li><li><a href=#benchmarking aria-label=Benchmarking.>Benchmarking.</a></li></ul></li></ul></div></details></div><div class=post-content><p>The blog serves as a backup for setting up the Nvidia Jetson Orion AGX for Development/Production.
Please note: review all the steps before proceeding.</p><p>I was updating from JetPack 5.x to 6.x. My Jetson Orion AGX had already been flashed.</p><h1 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h1><p>The blog covers installing the <a href=https://developer.nvidia.com/sdk-manager>Nvidia sdk manager</a>.
Make sure to install Ubuntu 22.04, not 24.04, for installing JetPack 6 (as of August 19, 2024).
Similarly, upgrade the host machine, install all the necessary Nvidia drivers and CUDA on the host machine before installing the Nvidia SDK Manager. More information on installing <a href=https://shekharkoirala.com.np/posts/nvidia_cuda12_ubuntu/>Nvidia Drivers on ubuntu</a></p><h2 id=prerequistics>Prerequistics<a hidden class=anchor aria-hidden=true href=#prerequistics>#</a></h2><p>It is also advisable to review the dedicated <a href=https://developer.nvidia.com/embedded/learn/jetson-agx-orin-devkit-user-guide/index.html#about-this-document>developer kit user guide</a>
Go over Jetson Zoo and check for package versions if needed: <a href=https://elinux.org/Jetson_Zoo>ZOO</a></p><h2 id=prepare-the-nvidia-jetson-device>Prepare the Nvidia Jetson Device<a hidden class=anchor aria-hidden=true href=#prepare-the-nvidia-jetson-device>#</a></h2><p>Once you log into the Nvidia SDK Manager, you can&rsquo;t verify the target board.
To do that, follow these steps:</p><ol><li>Connect the USB Type-C port located above the power jack to the power source.</li><li>Connect the Type-C port located beside the GPIO pins to the host computer.</li><li>Connect the Micro-USB [UART] port located beside the HDMI pins to the host computer.</li></ol><p>After this, make sure you boot the Nvidia Jetson in recovery mode. You can put the device in recovery mode in two ways:</p><ol><li><p>Press the middle button (between power and restart) and the restart button together, then release them together.
[This step didn&rsquo;t work for me.]</p></li><li><p>Turn on the Nvidia Jetson machine, open the terminal, and type:</p></li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo reboot --force forced-recovery
</span></span></code></pre></td></tr></table></div></div><p>After this the nvidia board will be seen in the Nvidia SDK Manager. You can also check it via command in host machine.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>lsusb
</span></span></code></pre></td></tr></table></div></div><h2 id=nvidia-sdk-manager>Nvidia SDK Manager<a hidden class=anchor aria-hidden=true href=#nvidia-sdk-manager>#</a></h2><p>Once the target board is detected, select the drive location where you want to download the necessary files.
Then, a prompt will appear to flash the board. Flash your board by selecting the eMMC drive.
Once the flash process is completed, it will prompt you to proceed with the installation of the Nvidia SDK Manager.
However, I encountered an issue where it wouldn&rsquo;t allow me to install everything.</p><p>Here‚Äôs a workaround: Open the terminal on the freshly flashed Nvidia board.</p><p>Connect to Wi-Fi, because the flashed Nvidia board functions like a new computer. Install the necessary software and drivers to proceed.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt update <span class=o>&amp;&amp;</span> sudo apt upgrade
</span></span><span class=line><span class=cl>sudo apt dist-upgrade
</span></span><span class=line><span class=cl>sudo apt install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
</span></span><span class=line><span class=cl>sudo apt install python3-pip
</span></span><span class=line><span class=cl>sudo apt install nvidia-jetpack
</span></span></code></pre></td></tr></table></div></div><p>And now, voila! You have the latest version of Nvidia Jetson.</p><h2 id=installing-python-packages>Installing python packages.<a hidden class=anchor aria-hidden=true href=#installing-python-packages>#</a></h2><p>Installing Python packages, including Poetry, can be tricky because TensorRT is installed in the system. To use it, you need to run the following command:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>poetry config virtualenvs.options.system-site-packages <span class=nb>true</span> 
</span></span></code></pre></td></tr></table></div></div><p>Yes, TensorRT is included in JetPack. You can verify its installation using the following command:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt-get install tensorrt nvidia-tensorrt-dev python3-libnvinfer-dev
</span></span></code></pre></td></tr></table></div></div><p>Next, install Torch, TorchVision, and ONNX Runtime.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>wget https://nvidia.box.com/shared/static/mp164asf3sceb570wvjsrezk1p4ftj8t.whl -O torch-2.3.0-cp310-cp310m-linux_aarch64.whl
</span></span><span class=line><span class=cl>pip install torch-2.3.0-cp310-cp310m-linux_aarch64.whl
</span></span></code></pre></td></tr></table></div></div><p>for the wheel file url check this.</p><ol><li><a href=https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048>forum posts</a> , found both torch and torchvision, but might not update later.</li><li><a href=https://elinux.org/Jetson_Zoo>jetson zoo</a> I didn&rsquo;t torch here</li><li><a href=https://developer.download.nvidia.com/compute/redist/jp/>jetson distribution</a> only found torch not torchvision.</li></ol><p>Yes, it‚Äôs a bit of a mess, but it works. We don‚Äôt have much flexibility with the Torch version‚Äîwe have to use whatever Nvidia provides for the JetPack versions.</p><h2 id=benchmarking>Benchmarking.<a hidden class=anchor aria-hidden=true href=#benchmarking>#</a></h2><p>Now, use the script to benchmark the speed of Torch and TensorRT.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>onnx</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>onnx_tensorrt.backend</span> <span class=k>as</span> <span class=nn>backend</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorrt</span> <span class=k>as</span> <span class=nn>trt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define a simple PyTorch model</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MyModel</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pool</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span> <span class=o>*</span> <span class=mi>16</span> <span class=o>*</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu3</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pool</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span> <span class=o>*</span> <span class=mi>16</span> <span class=o>*</span> <span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Export the PyTorch model to ONNX</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>MyModel</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>input_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>input_names</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;input&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>output_names</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>dummy_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>onnx</span><span class=o>.</span><span class=n>export</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>dummy_input</span><span class=p>,</span> <span class=s1>&#39;my_model.onnx&#39;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>input_names</span><span class=o>=</span><span class=n>input_names</span><span class=p>,</span> <span class=n>output_names</span><span class=o>=</span><span class=n>output_names</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load the ONNX model and create a TensorRT engine from it</span>
</span></span><span class=line><span class=cl><span class=n>model_onnx</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;my_model.onnx&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>engine</span> <span class=o>=</span> <span class=n>backend</span><span class=o>.</span><span class=n>prepare</span><span class=p>(</span><span class=n>model_onnx</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;CUDA:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a context for executing inference on the engine</span>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>engine</span><span class=o>.</span><span class=n>create_execution_context</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Allocate device memory for input and output buffers</span>
</span></span><span class=line><span class=cl><span class=n>input_name</span> <span class=o>=</span> <span class=s1>&#39;input&#39;</span>
</span></span><span class=line><span class=cl><span class=n>output_name</span> <span class=o>=</span> <span class=s1>&#39;output&#39;</span>
</span></span><span class=line><span class=cl><span class=n>input_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>input_buf</span> <span class=o>=</span> <span class=n>trt</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>alloc_cuda_pinned_memory</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>volume</span><span class=p>(</span><span class=n>input_shape</span><span class=p>)</span> <span class=o>*</span> <span class=n>trt</span><span class=o>.</span><span class=n>float32</span><span class=o>.</span><span class=n>itemsize</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_buf</span> <span class=o>=</span> <span class=n>trt</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>alloc_cuda_pinned_memory</span><span class=p>(</span><span class=n>trt</span><span class=o>.</span><span class=n>volume</span><span class=p>(</span><span class=n>output_shape</span><span class=p>)</span> <span class=o>*</span> <span class=n>trt</span><span class=o>.</span><span class=n>float32</span><span class=o>.</span><span class=n>itemsize</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load the PyTorch model into memory and measure inference speed</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;my_model.pth&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>num_iterations</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=n>total_time</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>input_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_data</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>total_time</span> <span class=o>+=</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl><span class=n>pytorch_fps</span> <span class=o>=</span> <span class=n>num_iterations</span> <span class=o>/</span> <span class=n>total_time</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;PyTorch FPS: </span><span class=si>{</span><span class=n>pytorch_fps</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a TensorRT engine from the ONNX model and measure inference speed</span>
</span></span><span class=line><span class=cl><span class=n>trt_engine</span> <span class=o>=</span> <span class=n>backend</span><span class=o>.</span><span class=n>prepare</span><span class=p>(</span><span class=n>model_onnx</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;CUDA:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>num_iterations</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=n>total_time</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>input_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_shape</span><span class=p>)</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>output_data</span> <span class=o>=</span> <span class=n>trt_engine</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>input_data</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>())[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>total_time</span> <span class=o>+=</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span>
</span></span><span class=line><span class=cl><span class=n>tensorrt_fps</span> <span class=o>=</span> <span class=n>num_iterations</span> <span class=o>/</span><span class=n>total_time</span>
</span></span><span class=line><span class=cl><span class=n>tensorrt_fps</span> <span class=o>=</span> <span class=n>num_iterations</span> <span class=o>/</span> <span class=n>total_time</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;TensorRT FPS: </span><span class=si>{</span><span class=n>tensorrt_fps</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Speedup: </span><span class=si>{</span><span class=n>tensorrt_fps</span><span class=o>/</span><span class=n>pytorch_fps</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>x&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Output:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>model-train-py3.10<span class=o>)</span> jetson-dublin@ubuntu:~/Development/shekhar/model_train$ python benchmark.py 
</span></span><span class=line><span class=cl>PyTorch  Inference Time: 0.007773 seconds
</span></span><span class=line><span class=cl>TensorRT Inference Time: 0.002509 seconds
</span></span><span class=line><span class=cl>TensorRT is 3.10x faster than PyTorch
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://shekharkoirala.github.io/tags/nvidia/>Nvidia</a></li><li><a href=https://shekharkoirala.github.io/tags/cuda/>CUDA</a></li><li><a href=https://shekharkoirala.github.io/tags/nvidia-sdk-manager/>Nvidia SDK Manager</a></li><li><a href=https://shekharkoirala.github.io/tags/jetson/>Jetson</a></li><li><a href=https://shekharkoirala.github.io/tags/pyenv/>Pyenv</a></li><li><a href=https://shekharkoirala.github.io/tags/poetry/>Poetry</a></li><li><a href=https://shekharkoirala.github.io/tags/tensorrt/>Tensorrt</a></li><li><a href=https://shekharkoirala.github.io/tags/benchmark/>Benchmark</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on x" href="https://x.com/intent/tweet/?text=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board&amp;url=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f&amp;hashtags=Nvidia%2cCUDA%2cNvidiaSDKManager%2cJetson%2cpyenv%2cpoetry%2ctensorrt%2cbenchmark"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f&amp;title=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board&amp;summary=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board&amp;source=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f&title=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on whatsapp" href="https://api.whatsapp.com/send?text=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board%20-%20https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on telegram" href="https://telegram.me/share/url?text=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board&amp;url=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Setting up Nvidia SDK Manager and Torch Library in Jetson Board on ycombinator" href="https://news.ycombinator.com/submitlink?t=Setting%20up%20Nvidia%20SDK%20Manager%20and%20Torch%20Library%20in%20Jetson%20Board&u=https%3a%2f%2fshekharkoirala.github.io%2fposts%2fnvidia_jetson_sdk_jetpack_setup%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>